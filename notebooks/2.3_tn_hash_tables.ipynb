{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEB SERVICE EXAMPLE\n",
    "**Goal** Count the number of times different IPs access my servers during an hour window. \n",
    "<div style=\"column-count: 2;\">\n",
    "\n",
    "  <div align=\"center\">\n",
    "    <img src=\"images\\web_service.png\" width=\"600\" style=\"display: block; margin-bottom: 20px;\">\n",
    "  </div>\n",
    "\n",
    "  <div align=\"center\">\n",
    "    <img src=\"images\\ip_logs.png\" width=\"600\" style=\"display: block; margin-bottom: 20px;\">\n",
    "  </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Tables\n",
    "**Use-cases:**\n",
    "* Python dictionaries\n",
    "* file systems\n",
    "* password verification\n",
    "* storage optimisation\n",
    "\n",
    "\n",
    "**Definition:** A generalisation of a queue where each element is assigned a priority and elements come out in order by priority.\n",
    "\n",
    "#### Operations for Web Address Problem\n",
    "| Operation      | Output | Time (array: direct addressing) | Time (list) | Time (hash)\n",
    "| :-            | :-    | :-: | :-: | :-: |\n",
    "`UpadeAccessList(p)`  <br> • $c = \\text{length of longest chain}$|   ...  | $O(1)$ | $O(1)$ | $O(c+1)$ \n",
    "`AccessedLastHour()`  <br> • $c = \\text{length of longest chain}$|   ...  | $O(1)$ | $O(n)$ | $O(c+1)$ \n",
    "**Required Memory** <br> • $n = \\text{\\#active IPs}$ <br> • $N = \\text{\\#all possible IPs}$ <br> • $m = \\text{cardinality of hash function}$ |   ...  | $O(N)$ <br> • Need $2^{32}$ memory for few IPs <br> • IPv6: $2^{128}$ won't fit in memory | $O(n)$ | $O(n+m)$ \n",
    "\n",
    "We want to make $m$ and $c$ as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Function\n",
    "**Definition:** For any set of objects $S$ and any integer $m>0$, a function $h : S \\to \\{0, 1, \\dots , m-1\\}$ is called a **hash function**.  \n",
    "**Definition:** $m$ is called the **cardinality** of hash function $h$.  \n",
    "**Definition:** When $h(o_1) = (o_2)$ and $o_1 \\neq o_2$, this is called a **collision**.\n",
    "\n",
    "**Desirable properties:**\n",
    "* $h$ should be fast to compute\n",
    "* different values for different objects\n",
    "* direct addressing with $O(m)$ memory\n",
    "* want small cardinality $m$\n",
    "* impossible to have all different values if number of objects $|S|$ is more than $m$.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"_.png\" width=\"450\" style=\"display: inline-block; margin-right: 0px;\">\n",
    "</p>\n",
    "\n",
    "### Map\n",
    "**Definition:** A **Map** from $S$ to $V$ is a data structure with methods `HasKey(O)`, `Get(O)`, `Set(O)`, where $O \\in S, v \\in V$.\n",
    "\n",
    "Store mapping from objects to other objects:\n",
    "* Filename $\\to$ file on disk\n",
    "* Student ID $\\to$ student name\n",
    "* Contact name $\\to$ contact phone number\n",
    "\n",
    "\n",
    "### Chaining\n",
    "Chaining is a technique to implement a hash table.  \n",
    "\n",
    "$h : S \\to \\{0, 1, \\dots , m-1\\}$  \n",
    "$O, O' \\in S$  \n",
    "$v, v' \\in V$  \n",
    "$A \\gets$ array of $m$ lists (chains) of pairs $(O, v)$  \n",
    "<p align=\"left\">\n",
    "    <img src=\"images\\chaining.png\" width=\"450\" style=\"display: inline-block; margin-right: 0px;\">\n",
    "</p>\n",
    "\n",
    "### Set\n",
    "\n",
    "**Definition:** A **Set** is a data structure with methods `Add(O)`, `Remove(O)`, `Find(O)`.  \n",
    "\n",
    "$h : S \\to \\{0, 1, \\dots , m-1\\}$  \n",
    "$O, O' \\in S$  \n",
    "$A \\gets$ array of $m$ lists (chains) of objects $O$ \n",
    "\n",
    "**EXAMPLES**\n",
    "* IPs accessed during last hour\n",
    "* Students on cmapus\n",
    "* Keyword in a programming language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Table\n",
    "\n",
    "**Definition:** An implementation of a **set** or a **map** using hashing is called a hash table.\n",
    "\n",
    "**Set:**\n",
    "* `unordered_set` in C++\n",
    "* `HashSet` in Java\n",
    "* `set` in Python\n",
    "\n",
    "**Map:**\n",
    "* `unordered_map` in C++\n",
    "* `HashMap` in Java\n",
    "* `dict` in Python\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone Book Problem\n",
    "Design a data structure to store your contacts: names of people along with their phone numbers. The data structure should be able to do the following quickly:\n",
    "* Add and delete contacts,\n",
    "* Lookup the phone number by name,\n",
    "* Determine who is calling given their phone number.\n",
    "\n",
    "We need implement two Maps as hash tables.  \n",
    "1. $\\text{phone number} \\to \\text{name}$  \n",
    "`int(123-45-67) = 1234567`\n",
    "\n",
    "2. $\\text{name} \\to \\text{phone number}$\n",
    "\n",
    "#### Parameters\n",
    "* $n$: #phone numbers stored\n",
    "* $m$: cardinality of the hash function\n",
    "* $c$: length of the lengest chain\n",
    "* $O(n+m)$: memory used\n",
    "* $\\alpha = \\frac{n}{m}$: load factor\n",
    "* $O(c+1)$: run time of operations  \n",
    "\n",
    "Want to make $m$ and $c$ small!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Function\n",
    "Ley $U$ be the **universe** - the set of all possible keys.  \n",
    "A set of hash functions \n",
    "$$\n",
    "\\mathcal{H} = \\{h:U \\to \\{0, 1, 2, \\dots , m-1 \\} \\}\n",
    "$$\n",
    "\n",
    " is called a **universal family** if for any two keys $x, y, \\in U, x \\neq y$ the probability of **collision**\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[h(x) = h(y)] \\leq \\frac{1}{m}\n",
    "$$\n",
    "\n",
    "**LEMMA**  \n",
    "If $h$ is chosen randomly from a universal family, the average length of the longest chain $c$ is $O(1+\\alpha)$, where $\\alpha$ is the load factor of the family.\n",
    "\n",
    "**COROLLARY**  \n",
    "If $h$ is from a universal family, operations with hash table run on average in time $O(1 + \\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Hash Table Size\n",
    "* Ideally, $0.5 < \\alpha < 1$\n",
    "* Use $O(m) = O(\\frac{n}{\\alpha}) = O(n)$ memory to store $n$ keys\n",
    "* Operations run in time $O(1 + \\alpha) = O(1)$ on average\n",
    "\n",
    "What if $n$ **unknown**?  \n",
    "Copy the idea of dynamic arrays:\n",
    "1. Resize the hash table when $\\alpha$ becomes too large.\n",
    "2. Choose new hash function and **rehash** all the objects.\n",
    "\n",
    "Similarly to dynamic arrays, single rehashing takes $O(n)$ time, but amortized running time of eachc operation with hash table is still $O(1)$ on average, because rehashing will be rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Integers - 1. $\\text{phone number} \\to \\text{name}$  \n",
    "1. Take phone numbers up to length $L=7$, e.g. `148-25-67`.\n",
    "2. Convert phone numbers to integers from $0$ to $10^L-1 = 9 \\; 999 \\; 999$:  \n",
    "`148-25-67` $\\to$ `1482567`\n",
    "3. Choose prime number bigger than $10^L$, e.g. `p = 10000019`.\n",
    "4. Choose hash table size, e.g. `m = 1000`.\n",
    "\n",
    "**LEMMA**  \n",
    "The family of hash functions \n",
    "$$\n",
    "\\mathcal{H}_p = \\{h_p^{a,b}\\}(x) = ((ax+b)\\mod p) \\mod m \\}, \\quad a \\in [1,p], b \\in [0, p-1]\n",
    "$$\n",
    "is a **universal family**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Strings - 2. $\\text{name} \\to \\text{phone number}$  \n",
    "1. Given a string $S$, convert each character $S[i]$ to integer code: ASCII code, Unicode, etc.\n",
    "2. Choose a big prime number.\n",
    "3. Randomly select a hash function $h_x^p$ from $\\mathcal{P}_p$.\n",
    "4. Randomly select a hash function $h_p^{a,b} from \\mathcal{H}_p$.\n",
    "4. Concatenate the two functions to get the hash function $h_m$.\n",
    "\n",
    "**LEMMA**  \n",
    "The family of polynomial hash functions \n",
    "$$\n",
    "\\mathcal{P}_p = \\left\\{h_p^x(S) = \\sum_{i=0}^{|S|-1} S[i]x^i \\mod p) \\right\\}, \\quad p \\; \\text{prime}, x \\in [1,p-1]\n",
    "$$\n",
    "has probability of **collsion** $\\leq \\frac{L}{p}$ (for any randomly selected hash function).\n",
    "\n",
    "**LEMMA**  \n",
    "For any two different strings $s_1$ and $s_2$ of length at most $L+1$ and cardinality $m$, the probability of **collsion** $ \\mathbb{P}[h_m(s_1) = h_m(s_2)] \\leq \\frac{1}{m} + \\frac{L}{p}$.\n",
    "\n",
    "**COROLLARY**  \n",
    "If $p > ML$ for any two different strings of lengths at most $L+1$m the probability of **collision** $\\mathbb{P}[h_m(s_1) = h_m(s_2)] = O(\\frac{1}{m})$.\n",
    "\n",
    "\n",
    "**RUNNING TIME**  \n",
    "* For big enough $p$, again $c = O(1 + \\alpha)$\n",
    "* Computing `PolyHash(S)` runs in time $O(|S|)$.\n",
    "* If lengths of the names in the phone book are bounded by constant $L$, computing $h(S)$ takes $O(L) = O(1)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a text $T$ (book, website, Facebook profile) and a pattern $P$ (word, phrase, sentence), find all occurances of $P$ in $T$.\n",
    "\n",
    "**Examples**  \n",
    "* Your name on a website\n",
    "* Twitter messages about your company\n",
    "* Detect files infected by virus - code patterns\n",
    "\n",
    "**Definition:** Denote $S[i..j]$ the substring of $S$ starting in position $i$ and ending in position $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: Find Pattern in Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input:** Strings $T$ and $P$.\n",
    "\n",
    "**Output:** All positions $i \\in [0, \\: |T| - |P|]$ such that $T[i \\: .. \\: i + |P|-1] = P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm      | Running Time\n",
    "| :-            | :- \n",
    "| `FindPatternNaive(T,P)` | $\\Theta(\\|T\\|\\|P\\|)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def AreEqual(S1, S2):\n",
    "    \"\"\"Running time = O(m)\"\"\"\n",
    "    if len(S1) != len(S2):\n",
    "        return False\n",
    "    for i in range(len(S1)):\n",
    "        if S1[i] != S2[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def FindPatternNaive(T, P):\n",
    "    \"\"\"Running time = O((n-m+1)*m) = O(n*m)\"\"\"\n",
    "    result = []\n",
    "    n = len(T)\n",
    "    m = len(P)\n",
    "    for i in range(n - m + 1):\n",
    "        if AreEqual(T[i:i+m], P):\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "FindPatternNaive(\"a cat and a dog\", \"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Need to compare $P$ with all substrings $S$ of $T$ of length $|P|$.\n",
    "* IDea: use hashing to quickly compare $P$ with substrings of $T$.\n",
    "\n",
    "\n",
    "### Rabin-Karp's Algorithm\n",
    "1. Use polynomial hash family $\\mathcal{P}_p$ with big prime $p$\n",
    "2. If $h(P) \\neq$ h(S)$, then definitely $P \\neq S$\n",
    "3. If $h(P) = h(S)$, call `AreEqual(P,S)`\n",
    "4. If equal, append index to return array.\n",
    "\n",
    "\n",
    "* If $P \\neq S$, the probability $\\mathbb{P}[h(P) = h(S)]$ is at most $\\frac{|P|}{P}$ for polynomial hashing.\n",
    "* On average, the total number of false alarms will be $(|T|-|T|+1)\\frac{|P|}{p}$, which can be made small by selecting $p \\gg |T||P|$.\n",
    "\n",
    "**Running time**\n",
    "* $h(P)$ is computed in $O(|P|)$\n",
    "* $h(T[i \\: .. \\: i + |P|-1])$ is computed in $O(|P|)$, $|T| - |P| +1$ times\n",
    "* `AreEqual` is computed in $O(|P|)$\n",
    "* If number of flase alarms is negligible and $P$ found $q$ times in $T$, then  `AreEqual` is computed $q + \\frac{(|T|-|P|+1)|P|}{p}$ times\n",
    "* Total time spent in `AreEqual` is $O((q + \\frac{(|T|-|P|+1)|P|}{p})|P|) = (q|P|)$ for $p \\ \\gg |T||P|$\n",
    "* Total running time is $O(|P|) + O((|T| - |P| +1)|P|) + (q|P|) = O(|T||P|)$\n",
    "\n",
    "**Improving running time by precomputing hashes**  \n",
    "* $h(S) = \\sum_{i=0}^{|S| - 1} S[i]x^i \\mod p$\n",
    "* $H[i] = h(T[i \\: .. \\: i + |P|-1]) = \\sum_{j=i}^{i+|P| - 1} T[j]x^{j-i} \\mod p$\n",
    "\n",
    "We can rewrite\n",
    "\n",
    "$$\n",
    "H[i] = xH[i+1] + (T[i] - T[i + |P|]x^{|P|}) \\mod p\n",
    "$$\n",
    "\n",
    "So, we can\n",
    "1. Compute $H[|T|-|P|]$ in time $O(|P|)$ using `PolyHash`\n",
    "2. Compute $x^{|P|}$ in time $O(|P|)$\n",
    "3. Use the recursive formula to compute $H[i]$ for all $i \\in [0, \\; |T|-|P|-1]$ in time $O(|T|-|P|)$\n",
    "\n",
    "Total time to precompute $H$ is $O(|T|+|P|)$\n",
    "\n",
    "**Improved running time**\n",
    "1. Compute $h(P)$ in time $O(|P|)$\n",
    "2. Precompute hashes in time $O(|T|+|P|)$\n",
    "3. Total time in `AreaEqual` is $O(q|P|)$ on average\n",
    "4. Total average running time is $O(|T| + (q+1)|P|) \\ll O(|T||P|)$ as $q$ is usually small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RabinKarp(T,P):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instant Uploads and Storage Optimization in Dropbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STORAGE OPTIMIZATION**  \n",
    "\n",
    "Consider a storage platform, e.g. OneDrive, Google Drive or Dropbox.\n",
    "\n",
    "When three different users upload the same file (e.g. Nyan Cat video), store only 1 copy, linking user files to that one copy.\n",
    "\n",
    "#### A. Naive Comparison\n",
    "1. Upload new file\n",
    "2. Go through all stored files\n",
    "3. Compare each stored file with new file byte-by-byte\n",
    "4. If there's the same file, store a link to it instead of the new file\n",
    "\n",
    "*Drawbacks*\n",
    "* Have to upload the file first anyway\n",
    "* $O(NS)$ to compare file of size $S$ with $N$ other files\n",
    "* $N$ grows, so total running time of uploads grows as $O(N^2)$\n",
    "\n",
    "#### B. Hash Comparison\n",
    "1. Upload new file and compute its hash\n",
    "2. Compare hashes (as in Rabin-Karp's algorithm)\n",
    "    - If hashed are different, files are different\n",
    "    -  If there's a file with the same hash, upload and compare directly\n",
    "\n",
    "*Drawbacks*\n",
    "* There can be collisions\n",
    "* Still have to upload the file to compare directly\n",
    "* Still have to compare with all $N$ stored files\n",
    "\n",
    "#### C. Several Hashes (Final Solution)\n",
    "0. Choose several (3 to 5) different hash functions (e.g. polynomial hashing with different $p$ or $x$) and compute all hashes for all files\n",
    "2. Upload new file and compute its hashes (locally before upload)\n",
    "2. Compare hashes\n",
    "    - If hashed are different, files are different\n",
    "    - If there's a file with the same hash, don't upload new file\n",
    "\n",
    "*Drawbacks*\n",
    "* Collisions can happen even for several hashes simultaneously. However,\n",
    "    * There are algorithms to find collisions for known hash functions\n",
    "    * Even for one hash function, collisions are extremely rare\n",
    "    * Using 3 or 5 hashes, you probably won't see a collision in a lifetime\n",
    "\n",
    "<br>\n",
    "\n",
    "* Still have to compare with $N$ already stored files\n",
    "    * When a file is submitted for upload, hashes are computed anyway, so\n",
    "    * store file addresses in a hash table.\n",
    "    * Also store all the hashes there.\n",
    "    * Only need the hashes to search in the table (we do not need the file itself)\n",
    "\n",
    "**MORE Problems**\n",
    "* Billions of files are uploaded daily\n",
    "* Trillions stored already\n",
    "* Too big for a simple hash table\n",
    "* Millions of users upload simultaneously\n",
    "* Too many requests for a single table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Need to store trillions or more objects, e.g.g file addresses, user profiles, emails\n",
    "* Need fast search/access\n",
    "* Hash tables provide $O(1)$ search/access on average, but for $n=10^{12}$, $O(n+m)$ memory becomes too big to store on one computer.\n",
    "\n",
    "Solution: **distributed hash tables**.\n",
    "\n",
    "### Distributed Hash Tables\n",
    "1. Get 1000 computers\n",
    "2. Create a hash table on each of them\n",
    "3. Determine which computer \"owns\" object $O$ by using another hash: number $h(O) \\mod 1000$\n",
    "4. Send request to that computer, seach/modify in the local hash table\n",
    "\n",
    "**Problems**\n",
    "* Computers sometimes break, e.g.  \n",
    "computer breaks once in 2 years $\\implies$ one of 1000 computers breaks every day.\n",
    "* Therefore, store several copies of the data.\n",
    "    * Need to relocate the data from the borken computer\n",
    "    * Service grows, and new computers are added to the cluster\n",
    "    * $h(O) \\mod 1000$ no longer work! Therefore use **Consistent Hashing**\n",
    "\n",
    "\n",
    "**Consistent Hashing**\n",
    "* Choose hash function $h$ with cardinality $m$ and put numbers from $0$ to $m-1$ on a circle clockwise\n",
    "* Each object $O$ is then mapped to a point on the circle with number $h(O)$\n",
    "* Map computer IDs to the same circle: $\\text{compID} \\to$ point number $h(\\text{compID})$  \n",
    "* Each object is stored on the \"closest\" computer\n",
    "* Each computer stores all objects falling on some arc of the circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"column-count: 2;\">\n",
    "\n",
    "  <div align=\"center\">\n",
    "    <img src=\"images\\consistent_hashing_3.png\" width=\"600\" style=\"display: block; margin-bottom: 20px;\">\n",
    "    When a computer goes off, \"neighbours\" take its data\n",
    "  </div>\n",
    "\n",
    "  <div align=\"center\">\n",
    "    <img src=\"images\\consistent_hashing_4.png\" width=\"600\" style=\"display: block; margin-bottom: 20px;\">\n",
    "    When a new computer is added, it takes fata from the \"neighbours\"\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overlay Network**\n",
    "* Need to copy\\relocate data\n",
    "* How will a node know where to send the data?\n",
    "    * Each node will know a few neighbours\n",
    "    * For each key, each node will either store it or know some node \"closer\" to this key\n",
    "    * E.g., each node knows neighbours, $\\pm 1, \\pm 2, \\pm 4, \\pm 8, \\dots $ nodes and can get/send any key in $O(\\log n)$.\n",
    "\n",
    "\n",
    "**CONCLUSION**\n",
    "* Distributed Hash Tables (DHT) store Big Data on many computers\n",
    "* Consisten Hashing (CH) is one way to determine which computer stores which data\n",
    "* CH uses mapping of keys and computer IDs on a circle\n",
    "* Each computer stores a range of kets\n",
    "* Overlay Network is used to route the data to/from the right computer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
